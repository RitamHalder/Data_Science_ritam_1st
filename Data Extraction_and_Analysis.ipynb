{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824da3f0-96e0-48cb-9256-953973ad136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579ec243-b34c-493c-86b0-9320548bf36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:\\\\Users\\\\ritam\\\\Downloads\\\\Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3805d42a-65be-45bb-8650-72561761df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract title and text from a URL\n",
    "def extract_title_and_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            \n",
    "            # Extract the title and article text\n",
    "            title = soup.title.string if soup.title else \"No Title\"\n",
    "            article_text = soup.select_one('div.td-post-content.tagdiv-type').get_text()\n",
    "                        \n",
    "            # Here, you should adjust your specific HTML structure to extract the article text.\n",
    "            # Replace the following line with appropriate code for your HTML structure.\n",
    "            # Example: article_text = soup.find(\"div\", class_=\"article-content\").get_text()\n",
    "            \n",
    "            # Save the extracted data to a text file\n",
    "            with open(f\"{URL_ID}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"Title: {title}\\n\\n\")\n",
    "                file.write(f\"Article Text:\\n{article_text}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to fetch URL: {url}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL: {url}\\nError: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a40fb3-2fde-4855-865b-415941415a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved article for URL_ID: 123.0\n",
      "Extracted and saved article for URL_ID: 321.0\n",
      "Extracted and saved article for URL_ID: 2345.0\n",
      "Extracted and saved article for URL_ID: 4321.0\n",
      "Extracted and saved article for URL_ID: 432.0\n",
      "Extracted and saved article for URL_ID: 2893.8\n",
      "Extracted and saved article for URL_ID: 3355.6\n",
      "Extracted and saved article for URL_ID: 3817.4\n",
      "Extracted and saved article for URL_ID: 4279.2\n",
      "Extracted and saved article for URL_ID: 4741.0\n",
      "Extracted and saved article for URL_ID: 5202.8\n",
      "Extracted and saved article for URL_ID: 5664.6\n",
      "Extracted and saved article for URL_ID: 6126.4\n",
      "Extracted and saved article for URL_ID: 6588.2\n",
      "Extracted and saved article for URL_ID: 7050.0\n",
      "Extracted and saved article for URL_ID: 7511.8\n",
      "Extracted and saved article for URL_ID: 7973.6\n",
      "Extracted and saved article for URL_ID: 8435.4\n",
      "Extracted and saved article for URL_ID: 8897.2\n",
      "Extracted and saved article for URL_ID: 9359.0\n",
      "Extracted and saved article for URL_ID: 9820.8\n",
      "Extracted and saved article for URL_ID: 10282.6\n",
      "Extracted and saved article for URL_ID: 10744.4\n",
      "Extracted and saved article for URL_ID: 11206.2\n",
      "Failed to fetch URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Extracted and saved article for URL_ID: 12129.8\n",
      "Extracted and saved article for URL_ID: 12591.6\n",
      "Extracted and saved article for URL_ID: 13053.4\n",
      "Extracted and saved article for URL_ID: 13515.2\n",
      "Extracted and saved article for URL_ID: 13977.0\n",
      "Extracted and saved article for URL_ID: 14438.8\n",
      "Extracted and saved article for URL_ID: 14900.6\n",
      "Extracted and saved article for URL_ID: 15362.4\n",
      "Extracted and saved article for URL_ID: 15824.2\n",
      "Extracted and saved article for URL_ID: 16286.0\n",
      "Extracted and saved article for URL_ID: 16747.8\n",
      "Extracted and saved article for URL_ID: 17209.6\n",
      "Failed to fetch URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Extracted and saved article for URL_ID: 18133.2\n",
      "Extracted and saved article for URL_ID: 18595.0\n",
      "Extracted and saved article for URL_ID: 19056.8\n",
      "Extracted and saved article for URL_ID: 19518.6\n",
      "Extracted and saved article for URL_ID: 19980.4\n",
      "Extracted and saved article for URL_ID: 20442.2\n",
      "Extracted and saved article for URL_ID: 20904.0\n",
      "Extracted and saved article for URL_ID: 21365.8\n",
      "Extracted and saved article for URL_ID: 21827.6\n",
      "Extracted and saved article for URL_ID: 22289.4\n",
      "Extracted and saved article for URL_ID: 22751.2\n",
      "Extracted and saved article for URL_ID: 23213.0\n",
      "Extracted and saved article for URL_ID: 23674.8\n",
      "Extracted and saved article for URL_ID: 24136.6\n",
      "Extracted and saved article for URL_ID: 24598.4\n",
      "Extracted and saved article for URL_ID: 25060.2\n",
      "Extracted and saved article for URL_ID: 25522.0\n",
      "Extracted and saved article for URL_ID: 25983.8\n",
      "Extracted and saved article for URL_ID: 26445.6\n",
      "Extracted and saved article for URL_ID: 26907.4\n",
      "Extracted and saved article for URL_ID: 27369.2\n",
      "Extracted and saved article for URL_ID: 27831.0\n",
      "Extracted and saved article for URL_ID: 28292.8\n",
      "Extracted and saved article for URL_ID: 28754.6\n",
      "Extracted and saved article for URL_ID: 29216.4\n",
      "Extracted and saved article for URL_ID: 29678.2\n",
      "Extracted and saved article for URL_ID: 30140.0\n",
      "Extracted and saved article for URL_ID: 30601.8\n",
      "Extracted and saved article for URL_ID: 31063.6\n",
      "Extracted and saved article for URL_ID: 31525.4\n",
      "Extracted and saved article for URL_ID: 31987.2\n",
      "Extracted and saved article for URL_ID: 32449.0\n",
      "Extracted and saved article for URL_ID: 32910.8\n",
      "Extracted and saved article for URL_ID: 33372.6\n",
      "Extracted and saved article for URL_ID: 33834.4\n",
      "Extracted and saved article for URL_ID: 34296.2\n",
      "Extracted and saved article for URL_ID: 34758.0\n",
      "Extracted and saved article for URL_ID: 35219.8\n",
      "Extracted and saved article for URL_ID: 35681.6\n",
      "Extracted and saved article for URL_ID: 36143.4\n",
      "Extracted and saved article for URL_ID: 36605.2\n",
      "Extracted and saved article for URL_ID: 37067.0\n",
      "Extracted and saved article for URL_ID: 37528.8\n",
      "Extracted and saved article for URL_ID: 37990.6\n",
      "Extracted and saved article for URL_ID: 38452.4\n",
      "Extracted and saved article for URL_ID: 38914.2\n",
      "Extracted and saved article for URL_ID: 39376.0\n",
      "Extracted and saved article for URL_ID: 39837.8\n",
      "Extracted and saved article for URL_ID: 40299.6\n",
      "Extracted and saved article for URL_ID: 40761.4\n",
      "Extracted and saved article for URL_ID: 41223.2\n",
      "Extracted and saved article for URL_ID: 41685.0\n",
      "Extracted and saved article for URL_ID: 42146.8\n",
      "Extracted and saved article for URL_ID: 42608.6\n",
      "Extracted and saved article for URL_ID: 43070.4\n",
      "Extracted and saved article for URL_ID: 43532.2\n",
      "Extracted and saved article for URL_ID: 43994.0\n",
      "Extracted and saved article for URL_ID: 44455.8\n",
      "Extracted and saved article for URL_ID: 44917.6\n",
      "Extracted and saved article for URL_ID: 45379.4\n",
      "Extracted and saved article for URL_ID: 45841.2\n",
      "Extracted and saved article for URL_ID: 46303.0\n",
      "Extracted and saved article for URL_ID: 46764.8\n",
      "Extracted and saved article for URL_ID: 47226.6\n",
      "Extracted and saved article for URL_ID: 47688.4\n",
      "Extracted and saved article for URL_ID: 48150.2\n",
      "Extracted and saved article for URL_ID: 48612.0\n",
      "Extracted and saved article for URL_ID: 49073.8\n",
      "Extracted and saved article for URL_ID: 49535.6\n",
      "Extracted and saved article for URL_ID: 49997.4\n",
      "Extracted and saved article for URL_ID: 50459.2\n",
      "Extracted and saved article for URL_ID: 50921.0\n",
      "Extracted and saved article for URL_ID: 51382.8\n",
      "Extracted and saved article for URL_ID: 51844.6\n",
      "Extracted and saved article for URL_ID: 52306.4\n",
      "Extracted and saved article for URL_ID: 52768.2\n"
     ]
    }
   ],
   "source": [
    "# Define the save_directory outside of the for loop\n",
    "save_directory = \"C:\\\\MyArticles\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Set the current working directory to the save_directory\n",
    "os.chdir(save_directory)\n",
    "\n",
    "# Iterate through the URLs in the Excel file\n",
    "for index, row in df.iterrows():\n",
    "    URL_ID = row[\"URL_ID\"]\n",
    "    URL = row[\"URL\"]\n",
    "    \n",
    "    if extract_title_and_text(URL):\n",
    "        print(f\"Extracted and saved article for URL_ID: {URL_ID}\")\n",
    "\n",
    "# Restore the original working directory\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47df6aa3-5da6-4193-b4b7-9967f7098052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed426f-5a24-4d87-8bf3-9951e303029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ritam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f3d6918-09f1-42b9-a720-6cc6c7825e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99ddc0c1-125b-42d7-8f9d-f0a097fb2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'C:\\MyArticles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ed98110-a08d-4abe-bdb2-e236fc6ed0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a52eced8-b13a-4484-b285-07a0153d3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_to_try = ['utf-8', 'ISO-8859-1', 'latin-1', 'cp1252', 'utf-16']\n",
    "\n",
    "# Iterate through the files in the directory and try different encodings\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    file_content = file.read()\n",
    "                    file_contents.append(file_content)\n",
    "                break  # Stop trying encodings once successful\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try the next encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c7c3681-565a-4682-85f4-0ebce06c82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the list of file contents\n",
    "df = pd.DataFrame({'Text': file_contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "280fb7b4-d34a-4f5f-8acd-bfb86ca31f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: Will AI Replace Us or Work With Us? - B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Will machine replace the human in the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: How humans and machines are evolving to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: How machine learning will affect your b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: Rise of telemedicine and its Impact on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Title: An outlook of healthcare by the year 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Title: AI in healthcare to Improve Patient Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Title: What if the Creation is Taking Over the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Title: What Jobs Will Robots Take From Humans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Title: Will Machine Replace The Human in the F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text\n",
       "0    Title: Will AI Replace Us or Work With Us? - B...\n",
       "1    Title: Will machine replace the human in the f...\n",
       "2    Title: How humans and machines are evolving to...\n",
       "3    Title: How machine learning will affect your b...\n",
       "4    Title: Rise of telemedicine and its Impact on ...\n",
       "..                                                 ...\n",
       "107  Title: An outlook of healthcare by the year 20...\n",
       "108  Title: AI in healthcare to Improve Patient Out...\n",
       "109  Title: What if the Creation is Taking Over the...\n",
       "110  Title: What Jobs Will Robots Take From Humans ...\n",
       "111  Title: Will Machine Replace The Human in the F...\n",
       "\n",
       "[112 rows x 1 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28dfd7ba-8a0e-457e-a4bf-2da90c45c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filenames of the stopwords files\n",
    "stopword_filenames = [\"StopWords_Auditor.txt\", \"StopWords_Currencies.txt\", \"StopWords_DatesandNumbers.txt\", \"StopWords_Generic.txt\", \"StopWords_GenericLong.txt\", \"StopWords_Geographic.txt\", \"StopWords_Names.txt\"]\n",
    "\n",
    "# Load stopword lists\n",
    "stopword_sets = []\n",
    "\n",
    "for filename in stopword_filenames:\n",
    "    stopwords_file_path = f\"C:\\Stopwords/{filename}\"  # Replace with the actual path\n",
    "    with open(stopwords_file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        stopwords = set(file.read().splitlines())\n",
    "        stopword_sets.append(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32ca20c4-2bb4-4a6b-a714-2260b27c7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text using custom stopword lists\n",
    "def clean_text(text, custom_stopwords):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word.lower() not in custom_stopwords]\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7657637b-0bac-46ca-928f-87aced2cf821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    # Define a set of vowels\n",
    "    vowels = \"AEIOUaeiou\"\n",
    "    \n",
    "    # Initialize syllable count\n",
    "    syllables = 0\n",
    "    \n",
    "    # Handle cases where the word is None or empty\n",
    "    if not word:\n",
    "        return 0\n",
    "\n",
    "    # Initialize the previous character\n",
    "    prev_char = None\n",
    "    \n",
    "    # Iterate over each character in the word\n",
    "    for char in word:\n",
    "        if char is not None and prev_char is not None:\n",
    "            # Check if the character is a vowel and the previous character is not a vowel\n",
    "            if char in vowels and prev_char not in vowels:\n",
    "                syllables += 1\n",
    "        prev_char = char\n",
    "    \n",
    "    # Handle some exceptions\n",
    "    if word is not None and word.endswith(\"es\"):\n",
    "        syllables -= 1\n",
    "    if word is not None and word.endswith(\"ed\"):\n",
    "        syllables -= 1\n",
    "    \n",
    "    # Ensure that the syllable count is at least 1\n",
    "    return max(syllables, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c31388b-659d-4cfd-8c15-ae3455f5ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis\n",
    "def perform_sentiment_analysis(article_text, custom_stopwords):\n",
    "    words = clean_text(article_text, custom_stopwords)\n",
    "    \n",
    "    # Load positive and negative word lists\n",
    "    with open(\"C:\\MasterDictionary/positive-words.txt\", \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        positive_words = set(file.read().splitlines())\n",
    "    \n",
    "    with open(\"C:\\MasterDictionary/negative-words.txt\", \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        negative_words = set(file.read().splitlines())\n",
    "    \n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    return polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "02a4ac0c-c968-40ca-9119-54fad1b6144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform readability analysis\n",
    "def perform_readability_analysis(article_text, custom_stopwords):\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    \n",
    "    # Calculate average sentence length and percentage of complex words\n",
    "    total_words = 0\n",
    "    total_complex_words = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = clean_text(sentence, custom_stopwords)\n",
    "        total_words += len(words)\n",
    "        total_complex_words += sum(1 for word in words if len(word) > 2)  # Assuming words with > 2 characters are complex\n",
    "    \n",
    "    average_sentence_length = total_words / len(sentences)\n",
    "    percentage_complex_words = (total_complex_words / total_words) * 100\n",
    "    \n",
    "    # Calculate Fog Index\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    # Calculate average number of words per sentence\n",
    "    average_words_per_sentence = total_words / len(sentences)\n",
    "    \n",
    "    # Calculate complex word count\n",
    "    complex_word_count = total_complex_words\n",
    "    \n",
    "    # Calculate word count\n",
    "    word_count = total_words\n",
    "    \n",
    "    # Calculate syllable count per word\n",
    "    syllable_count_per_word = sum(count_syllables(word) for word in clean_text(article_text, custom_stopwords)) / len(clean_text(article_text, custom_stopwords))\n",
    "    \n",
    "    # Calculate personal pronouns count\n",
    "    personal_pronouns_count = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', article_text, flags=re.IGNORECASE))\n",
    "    \n",
    "    # Calculate average word length\n",
    "    total_characters = sum(len(word) for word in clean_text(article_text, custom_stopwords))\n",
    "    average_word_length = total_characters / total_words\n",
    "    \n",
    "    return (\n",
    "        average_sentence_length,\n",
    "        percentage_complex_words,\n",
    "        fog_index,\n",
    "        average_words_per_sentence,\n",
    "        complex_word_count,\n",
    "        word_count,\n",
    "        syllable_count_per_word,\n",
    "        personal_pronouns_count,\n",
    "        average_word_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e00341b0-800f-4286-9a89-f3a22b510121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritam\\AppData\\Local\\Temp\\ipykernel_9256\\690975325.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = results._append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Title  Positive_Score  \\\n",
      "0           Title: Will AI Replace Us or Work With Us?        0.397590   \n",
      "1    Title: Will machine replace the human in the f...        0.402597   \n",
      "2    Title: How humans and machines are evolving to...        0.384615   \n",
      "3    Title: How machine learning will affect your b...        0.535714   \n",
      "4    Title: Rise of telemedicine and its Impact on ...        0.520000   \n",
      "..                                                 ...             ...   \n",
      "107  Title: An outlook of healthcare by the year 20...        0.187500   \n",
      "108  Title: AI in healthcare to Improve Patient Out...        0.394495   \n",
      "109  Title: What if the Creation is Taking Over the...        0.191489   \n",
      "110  Title: What Jobs Will Robots Take From Humans ...        0.351852   \n",
      "111  Title: Will Machine Replace The Human in the F...        0.484536   \n",
      "\n",
      "     Negative_Score  Polarity_Score  Subjectivity_Score  \\\n",
      "0         -0.397590        0.397590            0.041814   \n",
      "1         -0.402597        0.402597            0.051436   \n",
      "2         -0.384615        0.384615            0.046045   \n",
      "3         -0.535714        0.535714            0.067715   \n",
      "4         -0.520000        0.520000            0.095877   \n",
      "..              ...             ...                 ...   \n",
      "107       -0.187500        0.187500            0.050874   \n",
      "108       -0.394495        0.394495            0.054014   \n",
      "109       -0.191489        0.191489            0.112440   \n",
      "110       -0.351852        0.351852            0.056162   \n",
      "111       -0.484536        0.484536            0.052775   \n",
      "\n",
      "     Average_Sentence_Length  Percentage_of_Complex_Words  Fog_Index  \\\n",
      "0                  25.126582                    70.327456  38.181615   \n",
      "1                  23.390625                    69.939880  37.332202   \n",
      "2                  18.822222                    70.956316  35.911415   \n",
      "3                  22.972222                    71.704958  37.870872   \n",
      "4                  13.037500                    82.550336  38.235134   \n",
      "..                       ...                          ...        ...   \n",
      "107                32.256410                    71.383148  41.455823   \n",
      "108                26.207792                    73.538157  39.898380   \n",
      "109                10.450000                    70.454545  32.361818   \n",
      "110                22.623529                    70.410816  37.213738   \n",
      "111                19.347368                    69.749728  35.638839   \n",
      "\n",
      "     Average_Number_of_Words_Per_Sentence Complex_Word_Count Word_Count  \\\n",
      "0                               25.126582               1396       1985   \n",
      "1                               23.390625               1047       1497   \n",
      "2                               18.822222                601        847   \n",
      "3                               22.972222                593        827   \n",
      "4                               13.037500                861       1043   \n",
      "..                                    ...                ...        ...   \n",
      "107                             32.256410                898       1258   \n",
      "108                             26.207792               1484       2018   \n",
      "109                             10.450000                589        836   \n",
      "110                             22.623529               1354       1923   \n",
      "111                             19.347368               1282       1838   \n",
      "\n",
      "     Syllable_Count_Per_Word Personal_Pronouns  Average_Word_Length  \n",
      "0                   1.513350                16             4.615113  \n",
      "1                   1.480294                21             4.502338  \n",
      "2                   1.492326                 7             4.570248  \n",
      "3                   1.444982                 2             4.388150  \n",
      "4                   2.239693                 2             6.613615  \n",
      "..                       ...               ...                  ...  \n",
      "107                 1.562798                10             4.852941  \n",
      "108                 1.663033                 1             5.160555  \n",
      "109                 1.711722                 7             5.338517  \n",
      "110                 1.607384                 3             4.913677  \n",
      "111                 1.479869                18             4.431447  \n",
      "\n",
      "[112 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "result_columns = [\n",
    "    'Title',\n",
    "    'Positive_Score',\n",
    "    'Negative_Score',\n",
    "    'Polarity_Score',\n",
    "    'Subjectivity_Score',\n",
    "    'Average_Sentence_Length',\n",
    "    'Percentage_of_Complex_Words',\n",
    "    'Fog_Index',\n",
    "    'Average_Number_of_Words_Per_Sentence',\n",
    "    'Complex_Word_Count',\n",
    "    'Word_Count',\n",
    "    'Syllable_Count_Per_Word',\n",
    "    'Personal_Pronouns',\n",
    "    'Average_Word_Length'\n",
    "]\n",
    "results = pd.DataFrame(columns=result_columns)\n",
    "\n",
    "# Iterate over each article and perform analysis\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Text']\n",
    "    # Extract the title based on the format \"Title: [Title Text]\"\n",
    "    title = text.split('-')[0].strip() if '-' in text else \"\"\n",
    "    article_text = text[len(title) + 1:].strip()  # Remove the extracted title from the text\n",
    "    \n",
    "    custom_stopwords = stopword_sets[index % len(stopword_sets)]  # Choose the custom stopwords for this article\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    polarity_score, subjectivity_score = perform_sentiment_analysis(article_text, custom_stopwords)\n",
    "    \n",
    "    # Perform readability analysis\n",
    "    (\n",
    "        average_sentence_length,\n",
    "        percentage_complex_words,\n",
    "        fog_index,\n",
    "        average_words_per_sentence,\n",
    "        complex_word_count,\n",
    "        word_count,\n",
    "        syllable_count_per_word,\n",
    "        personal_pronouns_count,\n",
    "        average_word_length\n",
    "    ) = perform_readability_analysis(article_text, custom_stopwords)\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    results = results._append({\n",
    "        'Title': title,\n",
    "        'Positive_Score': polarity_score,\n",
    "        'Negative_Score': -polarity_score,  # Convert to negative for Negative Score\n",
    "        'Polarity_Score': polarity_score,\n",
    "        'Subjectivity_Score': subjectivity_score,\n",
    "        'Average_Sentence_Length': average_sentence_length,\n",
    "        'Percentage_of_Complex_Words': percentage_complex_words,\n",
    "        'Fog_Index': fog_index,\n",
    "        'Average_Number_of_Words_Per_Sentence': average_words_per_sentence,\n",
    "        'Complex_Word_Count': complex_word_count,\n",
    "        'Word_Count': word_count,\n",
    "        'Syllable_Count_Per_Word': syllable_count_per_word,\n",
    "        'Personal_Pronouns': personal_pronouns_count,\n",
    "        'Average_Word_Length': average_word_length\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Print or save the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0fab3291-805d-4f96-aa3e-914e6eb4f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the absolute file path where you want to save the Excel file\n",
    "output_excel_file = \"C:\\MasterDictionary/Output_Data_Structure.xlsx\"\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results.to_excel(output_excel_file, index=False)  # Set index to False to exclude the index column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411f1cf-fc88-458d-a0bb-4b2fd795f553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
